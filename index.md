---
layout: homepage
---

## About Me

Hi! My name is Minhak Song (송민학), and I am an undergraduate student at [KAIST](https://www.kaist.ac.kr/en/), double majoring in Industrial & Systems Engineering and Mathematical Sciences. My research interest lies in building the theoretical foundations of modern machine learning through the lens of optimization theory and statistics. Recently, I have been studying the training dynamics of optimization algorithms in deep learning under the guidance of  [Prof. Chulhee Yun](https://chulheeyun.github.io). Currently, I am a visiting student at the [University of Washington](https://www.washington.edu/), hosted by [Prof. Simon Du](https://simonshaoleidu.com/), where I am working on theoretical aspects of reinforcement learning.

Feel free to reach out for discussions and potential collaborations!

## Research Interests

- **Machine Learning Theory**
- **Optimization**
- **Statistics**

## News

- **[Jan. 2025]** Our [paper](https://arxiv.org/abs/2405.16002) on SGD dynamics along Hessian eigenspaces is accepted to **ICLR 2025**.
- **[Jan. 2025]** I joined [Prof. Simon Du](https://simonshaoleidu.com/)'s group as a visiting student at the [Paul G. Allen School of Computer Science & Engineering](https://www.cs.washington.edu/) at the [University of Washington](https://www.washington.edu/).
- **[Jun. 2024]** Our [paper](https://arxiv.org/abs/2405.16002) on SGD dynamics along Hessian eigenspaces is accepted to **ICML 2024 Workshop** on High-dimensional Learning Dynamics: The Emergence of Structure and Reasoning.
- **[Jan. 2024]** Our [paper](https://arxiv.org/abs/2310.01082) on the optimization characteristics of linear Transformers is accepted to **ICLR 2024**.
- **[Oct. 2023]** Our [paper](https://arxiv.org/abs/2310.01082) on the optimization characteristics of linear Transformers is accepted to **NeurIPS 2023 Workshop** on Mathematics of Modern Machine Learning as an **oral presentation**.
- **[Sep. 2023]** Our [paper](https://arxiv.org/abs/2307.04204) on understanding the Edge of Stability phenomenon in deep learning is accepted to **NeurIPS 2023**.

{% include_relative _includes/publications.md %}

{% include_relative _includes/services.md %}
